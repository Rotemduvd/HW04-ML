{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Logistic Regression and Evaluation of Binary Classifiers\n",
    "\n",
    "\n",
    "### Make sure that you read and fully understand all the guidelines listed below before you proceed with the exercise.\n",
    "\n",
    "* HW assignments are a significant part of the learning experience in this course and contribute 50% to your final grade. So, make sure to devote the appropriate time to them.\n",
    "* **Sharing solutions with someone who is not your submitting partner is strictly prohibited**. This includes reading someone else's code or sharing your code / posting it somewhere.\n",
    "* Appeals regarding submissions that do not follow the guidelines will not be accepted. \n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "### Guidelines for Programming Exercises:\n",
    "\n",
    "* Complete the required functions in `hw4.py`. Any modifications to this notebook will not be tested by our automated tests.\n",
    "* Write **efficient**, **vectorized** code whenever possible. Some calculations in this exercise can take several minutes when implemented efficiently, but will take much longer otherwise. Unnecessary loops will result in point deductions.\n",
    "* You are responsible for the correctness of your code. You can add tests to this jupyter notebook to validate your solution. The contents of this jupyter notebook will not be graded or checked.\n",
    "* You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/), numpy and pandas only. **Do not import anything else.**\n",
    "* Use `numpy` version 1.15.4 or higher.\n",
    "* Your code must run without errors. Code that cannot run will not be graded.\n",
    "* Include docstrings and comments explaining your code\n",
    "* Your code will be tested using automated scripts. So, failure to follow the instructions may lead to test failure, which might significantly affect your grade. \n",
    "\n",
    "\n",
    "### Guidlines for Theoretical Exercises\n",
    "* Your solution should be written or typed and submitted in a separate file `hw4.pdf`.\n",
    "* If you scan a handwritten solution, make sure that your handwriting is legible and the scan quality is good.\n",
    "* You are expected to solve the questions analytically and provide a step-by-step solution. \n",
    "* It is okay and often recommended to use python to carry out the computations. \n",
    "* You may use the lecture slides and previous homework assignments as references, unless explicitly asked to prove a result from class. \n",
    "\n",
    "### Submission Guidelines:\n",
    "* Submit your solutiuon in a zip file that contains: \n",
    "  - The `hw4.py` script with your solution to the progamming exercise\n",
    "  - This notebook with your added tests (this is not checked or graded)\n",
    "  - The `hw4.pdf` file with your solution to the theoretical exercises.\n",
    "  \n",
    "* The name of the zip file should contain your ID(s). For example, `hw4_123456789_987654321.zip` if you submitted in pairs and `hw4_123456789.zip` if you submitted the exercise alone.\n",
    "* Please use **only a zip** file in your submission.\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Please sign that you have read and understood the instructions: \n",
    "\n",
    "### *** YOUR ID HERE ***\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Theoretical Exercise (16 points)\n",
    "\n",
    "In class we developed the logistic regression model by requiring that the class labeling probability (for class 1) follows is a sigmoid function of the inner product of the feature vector ($x$) and the weight vector ($w$):\n",
    "$$\n",
    "\\Pr[Y=1|X=x,w] ~~=~~ \\sigma(w^\\top x) ~~=~~ \\frac{1}{1+ e^{-w^\\top x}}\n",
    "$$\n",
    "Using this function for the class labeling probability, we defined the data likelihood, the binary cross entroy (BCE) loss, and a gradient descent algorithm for minimizing the BCE loss.\n",
    "\n",
    "In this question, you will derive a model and optimization scheme under the following assumptions on the class labeling probabilities:\n",
    "$$\n",
    "\\Pr[Y=1|X=x,w] = \\Phi(w^\\top x), \\qquad \\Phi(x) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^x e^{-t^2/2} dt. \n",
    "$$\n",
    "This is called the *probit probability model* (unlike the logistic probability model in logistic regression).\n",
    "Recall that $\\Phi(x)$ is the CDF of the standard normal variable (with mean 0 and variance 1).\n",
    "The probit model is similar to logistic regression but uses the cumulative normal distribution \n",
    "instead of the sigmoid function. Both models typically yield similar results in practice.\n",
    "\n",
    "\n",
    "1. Derive the log-likelihood $\\ell(w;D)$ under this model. You may assume that the marginal probability of the data, $P_X(\\{x^{(i)}\\}_{i=1}^n)$, does not depend on the weight vector $w$ (as we assumed in the logistic probability model).\n",
    "2. Express the problem of maximizing the log-likelihood in this model as a problem of minimizing the appropriate binary cross-entropy (BCE) loss. The BCE loss you specify here **should not** be normalized by the number of samples ($n$).\n",
    "3. Find the gradient of the BCE loss and describe how to minimize it using gradient descent.  \n",
    "\n",
    "Question 4-6 below are based on a question from a previous exam (Moed B 2024).\n",
    "In all three questions, consider the training data set $D=\\{x^{(i)},y^{(i)} \\}_{i=1}^{8}$ specified in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 1, -1, 2, 4], \n",
    "              [1, -1, -2, 1, 2],\n",
    "              [1, 2, 0, 3, 1],\n",
    "              [1, 0, 1, -1, 3], \n",
    "              [1, 1, -1, 2, 2], \n",
    "              [1, -1, -1, 2, 3],\n",
    "              [1, 2, 3, 1, 1],\n",
    "              [1, 2, 3, 1, 5]\n",
    "              ])\n",
    "y = np.array([1, 0, 1, 0, 1, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Should you add a \"bias\" term $x_0=1$? \n",
    "\n",
    "5. Assume initial weights `w = [0, 0, 0, 0, 0]` and learning rate $\\eta=0.01$. Implement `n_iter=4` steps of the gradient descent algorithm for the probit model. For each step, report the gradient and the BCE loss **after** the update. You should use python code to implement the computation and use `scipy.stats.norm.cdf` and `scipy.stats.norm.pdf` to evaluate the normal CDF and PDF. Use the unnormalized version of the BCE loss here (without dividing by the number of samples). No need to submit your code, just the output.\n",
    "\n",
    "6. Evaluate the model you obtained at the end of the last iteration above on the trianing set. What is the class label it predicts for each of the eight samples in the training set? On which of the samples does it make a classification error? What is its accuracy? As in (5) above, you should use python code to obtain the results, but you submit only your answers without the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Coding Assignment (84 points)\n",
    "\n",
    "In this coding assignment, you will implement logistic regression to classify handwritten digits from the MNIST dataset.\n",
    "We will focus on binary classification between digits 8 and 9, where the images have been corrupted with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Preparing Data (2 points)\n",
    "\n",
    "We start by loading the MNIST data and adding random noise to every image.\n",
    "Make sure the data file ``mnist_8n9.npz`` is in the working directory and then execute the code below.\n",
    "Each sample consists of 784 integer features in the range $[0,255]$ representing a $28 \\times 28$ grayscale image. \n",
    "The code below loads the data, normalizes the features to range [0,1] and adds random noise to each image.\n",
    "We then print nine random samples as bitmap images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adds random noise to given data (train or test)\n",
    "def add_noise(X, noise_level):\n",
    "    np.random.seed(42)\n",
    "    return np.minimum(X + (np.random.rand(X.shape[0], X.shape[1]) > noise_level), 1)\n",
    "\n",
    "# Load train and test data and normalize pixel values to [0, 1]\n",
    "data = np.load('mnist_8n9.npz')\n",
    "X_train = data[\"X_train\"] / 255\n",
    "X_test  = data[\"X_test\"] / 255\n",
    "y_train = data['y_train'].astype(int)\n",
    "y_test  = data['y_test'].astype(int)\n",
    "\n",
    "# add random noise to train and test data\n",
    "X_train = add_noise(X_train, noise_level=0.1)\n",
    "X_test  = add_noise(X_test, noise_level=0.1)\n",
    "\n",
    "# print basic stats\n",
    "print(\"Training set contains\", X_train.shape[0], \"samples with\", X_train.shape[1], \"features\")\n",
    "print(\"Test set contains\", X_test.shape[0], \"samples with\", X_test.shape[1], \"features\")\n",
    "\n",
    "# plot nine random images from the training set\n",
    "sample_indices = np.random.choice(X_train.shape[0], 9, replace=False)\n",
    "plt.figure(figsize=(5, 5))\n",
    "for j,i in enumerate(sample_indices):\n",
    "    plt.subplot(3, 3, j+1)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Label: {y_train[i]}', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `add_bias_term` in `hw4.py` to add a constant 1 as the 0th feature to each sample (2 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw4 import add_bias_term\n",
    "    \n",
    "X_train = add_bias_term(X_train)\n",
    "X_test  = add_bias_term(X_test)\n",
    "\n",
    "# Verify that the size of each instance in the datasets has increased by 1\n",
    "assert X_train.shape[1] == X_test.shape[1] == 785\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing The Logistic Regression Model (30 points)\n",
    "\n",
    "Implement the `LogisticRegressionGD` class in `hw4.py`.\n",
    "\n",
    "This class consists of a constructor (which is already implemented) and four member functions, which you should implement:\n",
    "`predict_proba`, `predict`, `BCE_loss`, and `fit`.\n",
    "\n",
    "Recall that the logistic regression model attempts to minimize the binary cross-entropy loss:\n",
    "$$\n",
    "BCE(w;D) = -\\frac{1}{n}\\sum_{i=1}^n [y_i \\log(\\sigma(w^\\top x^{(i)})) + (1-y_i)\\log(1-\\sigma(w^\\top x^{(i)}))] ~,\n",
    "$$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid function, $w$ are the model parameters,\n",
    "$x^{(i)}$ are the input feature vectors, and $y_i$ are the binary labels.\n",
    "Note that here we use the version of the BCE loss **normalized by the number of samples**.\n",
    "Make sure to use this version in your implementation.\n",
    "\n",
    "The code below fits a logistic regression model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw4 import LogisticRegressionGD\n",
    "\n",
    "model = LogisticRegressionGD(learning_rate=0.001, max_iter=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selecting A Learning Rate (10 points)\n",
    "\n",
    "Implement the function `select_learning_rate` in `hw4.py` to find an effective learning rate for the training data.\n",
    "\n",
    "This function receives as parameter an array of learning rates.\n",
    "For each learning rate, fit a logistic regression classifier to the training data using a pre-specified number of GD iterations and record the final BCE loss against the learning rate. Then, return the learning rate that resulted in the smallest loss.\n",
    "\n",
    "The code below selects a learning rate among a list of values and then trains a logistic regression model using this rate and 10,000 iterations. It then prints the resulting weights and reports their performance on the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw4 import select_learning_rate\n",
    "\n",
    "# Select the best learning rate based on the minimal BCE loss after 1000 GD iterations\n",
    "learning_rates = [0.5, 0.2, 0.1, 0.01, 0.005]\n",
    "max_iter = 1000\n",
    "learning_rate = select_learning_rate(X_train, y_train, learning_rates, max_iter=max_iter)\n",
    "print(f\"The learning rate attaining the minimal BCE after {max_iter} GD iterations is {learning_rate}. We pick it.\")\n",
    "\n",
    "# Train the model with the selected learning rate\n",
    "model = LogisticRegressionGD(learning_rate=learning_rate, max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Convert the weights to a 28x28 image for visualization and plot them\n",
    "weights = model.w_[1:].reshape(28, 28)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(weights)\n",
    "plt.axis('off')\n",
    "plt.title(\"weights of LoR classifier\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the training and test sets\n",
    "print(f\"Train BCE: {model.BCE_loss(X_train, y_train)}\")\n",
    "y_pred_train = model.predict(X_train)\n",
    "print(f\"Train accuracy: {np.sum(y_pred_train == y_train) / len(y_train)}\")\n",
    "\n",
    "print(f\"Test BCE: {model.BCE_loss(X_test, y_test)}\")\n",
    "y_pred_test = model.predict(X_test)\n",
    "print(f\"Test accuracy: {np.sum(y_pred_test == y_test) / len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating The Model Using Cross Validation (15 points)\n",
    "\n",
    "Implement the function `cv_accuracy_and_BCE_error` in `hw4.py` for computing the average model accuracy and loss using cross validation (CV).\n",
    "\n",
    "The purpose of this analysis is to be able to better approximate the test error from the training data. \n",
    "\n",
    "We would like to get a better assessment of the test error from the train data using cross-validation.\n",
    "The function partitions the (train) data to `n_folds=5` folds and used 5-fold CV to produce 5 estimates of accuracy and loss.\n",
    "The CV loss and accuracy is then obtained as the average of these 5 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw4 import cv_accuracy_and_bce_error\n",
    "\n",
    "cv_acc, cv_bce = cv_accuracy_and_bce_error(X_train, y_train, n_folds=5)\n",
    "print(\"Average CV BCE loss: \", np.mean(cv_bce))\n",
    "print(\"Average CV accuracy: \", np.mean(cv_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reporting Various Accuracy Metrics (12 points)\n",
    "\n",
    "In this section you will evaluate the model's performance with greater detail.\n",
    "We start by examining the *confusion matrix* of our binary classifier.\n",
    "Recall that this confusion matrix has the following structure:\n",
    "\n",
    "|                   |   |    True         |   Class    |\n",
    "|-------------------|---|-----------------|------------|\n",
    "|                   |   |  0              | 1          |\n",
    "| **Predicted**     | 0 | #TN             | #FN        |\n",
    "| $~~~~~~~$**Class**| 1 | #FP             | #TP        |\n",
    "\n",
    "The cells of this matrix contain counts of the following cases:\n",
    "- A *True Negative*  (TN) is a negative sample ($y=0$) that is   correctly classified as negative ($\\hat y=0$)\n",
    "- A *True Positive*  (TP) is a positive sample ($y=1$) that is   correctly classified as positive ($\\hat y=1$)\n",
    "- A *False Negative* (FN) is a positive sample ($y=1$) that is incorrectly classified as negative ($\\hat y=0$); also called a type II error\n",
    "- A *False Positive* (FP) is a negative sample ($y=0$) that is incorrectly classified as positive ($\\hat y=1$); also called a type I error\n",
    "\n",
    "These metrics are useful in real-world applications where different types of errors have different consequences.\n",
    "Consider a medical diagnosis system for detecting a serious disease:\n",
    "- A false positive (FP) means unnecessarily worrying a healthy patient and potentially prescribing unnecessary treatment\n",
    "- A false negative (FN) means missing a sick patient who needs treatment - potentially life-threatening\n",
    "\n",
    "In this case, we may wish to minimize FNs at the expense of potentially adding some FPs to minimize missed diagnoses, even at the cost of more false positives.\n",
    "\n",
    "We measure these errors using several key metrics. First, looking at errors **per true class**:\n",
    "Since the number of positive samples ($y=1$) is obtained by #TP + #FN and the number of negative samples ($y=0$) is obtained by #TN + #FP,\n",
    "we get the following rates:\n",
    "- *True Positive Rate* (TPR) = #TP / (#TP + #FN)  - the fraction of actual positive samples we correctly   classify as positive\n",
    "- *True Negative Rate* (TNR) = #TN / (#TN + #FP)  - the fraction of actual negative samples we correctly   classify as negative\n",
    "- *False Positive Rate* (FPR) = #FP / (#TN + #FP) - the fraction of actual negative samples we incorrectly classify as positive\n",
    "- *False Negative Rate* (FNR) = #FN / (#TP + #FN) - the fraction of actual positive samples we incorrectly classify as negative\n",
    "\n",
    "We also care about performance across the **entire dataset**:\n",
    "Since the total number of samples is #TP + #FP + #TN + #FN, we measure:\n",
    "- Accuracy = (#TP + #TN) / (#TP + #FP + #TN + #FN) - the fraction of correct predictions overall\n",
    "- Risk = (#FP + #FN) / (#TP + #FP + #TN + #FN)     - the fraction of incorrect predictions overall\n",
    "\n",
    "Finally, three especially useful composite metrics that data scientists rely on:\n",
    "  - *Recall* = TPR = #TP / (#TP + #FN) - measures the fraction of positive predictions out of all **actual positive samples**\n",
    "  - *Precision* = #TP / (#TP + #FP) - measures the fraction of positive predictions out of all  **positive predictions**\n",
    "  - *F1* = 2 * (precision * recall) / (precision + recall) - a balanced measure combining precision and recall (their harmonic mean)\n",
    "\n",
    "In our disease example, high recall would mean catching most sick patients (few false negatives),\n",
    "while high precision would mean most patients we flag as sick are actually sick (few false positives).\n",
    "The F1 score helps balance these competing objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `calc_and_print_metrics` in `hw4.py` which calculates the various measures mentioned above (12 points).\n",
    "\n",
    "Consider class \"9\" as positive ($y=1$) and class \"8\" as negative ($y=0$). \n",
    "\n",
    "The code below computes and prints the accuracy measures for the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the metrics for the LogisticRegression classifier.\n",
    "from hw4 import calc_and_print_metrics\n",
    "\n",
    "# Predict the labels for the training and test sets\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(f\"Training set (total = {y_train.shape[0]}):\")\n",
    "calc_and_print_metrics(y_train, y_pred_train, positive_class=9)\n",
    "\n",
    "print(f\"\\nTest set: (total = {y_test.shape[0]}):\")\n",
    "calc_and_print_metrics(y_test, y_pred_test, positive_class=9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Examining The Tradeoff Between Different Error Types Using Receiver Operating Characteristic (ROC) (15 points)\n",
    "\n",
    "An important feature of (linear) binary classifiers is the possibility to tune their sensitivity to different kinds of errors.\n",
    "In linear regression models this can easily be achieved by tuning the probability threshold for positive predictions (which can be seen as considering hyperplanes parallel to the original one).\n",
    "By default, we use a threshold of $\\frac 1 2$ and associate a sample $x$ for which $\\sigma(w^\\top x)>\\frac 1 2$ with positive prediction $\\hat y = 1$. By increasing this threshold, we decrease the number of positive predictions, which increases the number of false positives and decreases the number of false negatives.\n",
    "\n",
    "With the *receiver operating characteristic (ROC) curve* we examine the performance of a given binary classifier across different classification thresholds. For each threshold we plot the TPR (Y axis) against the FPR (X axis). This curve allows us to assess the classifier's performance across prediction thresholds:\n",
    "* A perfect classifier would have a point at (0,1), indicating that there is a threshold that achieves no false positives and all true positives.\n",
    "* A classifier for which the ROC curve follows close to the diagonal ($y=x$) is no better than a classifier that randomly selects a predicted label.\n",
    "\n",
    "Most classifiers fall somewhere in between the above two options. A common way to evaluate the classifier across all thresholds is using the  area under the curve (AUC). This provides a single measure that tells us where a classifier falls in the spectrum between a perfect classifier (AUC=1) and a random one (AUC=$\\frac 12$).\n",
    "\n",
    "Implement the function `fpr_tpr_per_threshold` in `hw4.py` which calculates the FPRs and TPRs of a given classifier using a list of thresholds (12 points).\n",
    "\n",
    "The code below uses the output of this function to plot a ROC curve and compute the AUC for a random classifier and for your logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw4 import fpr_tpr_per_threshold\n",
    "\n",
    "def illustrate_roc_curve_and_auc(fpr, tpr, label='ROC Curve'):\n",
    "    \"\"\"\n",
    "    Plot the ROC curve and calculate AUC score for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fpr : list or array\n",
    "        False Positive Rate values.\n",
    "    tpr : list or array\n",
    "        True Positive Rate values.\n",
    "    label : str, optional\n",
    "        Name of the classifier to display in plot legend. Default is 'ROC Curve'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the AUC score\n",
    "    auc = 0\n",
    "    # Sort the fpr and tpr arrays by tpr in descending order\n",
    "    idcs = np.argsort(tpr)[::-1]\n",
    "    sorted_fpr = np.array(fpr)[idcs]\n",
    "    sorted_tpr = np.array(tpr)[idcs]\n",
    "\n",
    "    for i in range(len(sorted_fpr)-1):\n",
    "        auc += (sorted_fpr[i] - sorted_fpr[i+1]) * sorted_tpr[i]\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{label} (AUC = {auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='FPR = TPR')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Random classifier probabilities\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "random_classification_probs = np.random.rand(len(y_test))\n",
    "# Compute FPRs and TPRs for the random classifier\n",
    "fpr , tpr = fpr_tpr_per_threshold(y_test, random_classification_probs, positive_class=9)\n",
    "# Plot ROC curve and compute AUC score\n",
    "illustrate_roc_curve_and_auc(fpr , tpr, label=f'Random Classifier (seed={seed})')\n",
    "# this classifier is equivalent to a random guess, so its FPR and TPR are about equal and its AUC is 0.5\n",
    "\n",
    "# Logistic regression classifier probabilities\n",
    "logistic_classifier_probs = model.predict_proba(X_test)\n",
    "# Compute FPRs and TPRs for the logistic regression model\n",
    "fpr , tpr = fpr_tpr_per_threshold(y_test, logistic_classifier_probs, positive_class=9)\n",
    "# Plot ROC curve and compute AUC score\n",
    "illustrate_roc_curve_and_auc(fpr , tpr, label=f'Logistic Regression')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
